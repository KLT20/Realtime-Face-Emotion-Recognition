1. Problem statement: 
The problem consists in recognizing emotions from pictures of faces 
provided to the model. There are 7 emotions, labeled from 0 to 6: happiness, 
neutral, sadness, anger, surprise, disgust, fear, and the images are downloaded 
in a csv file as 48x48 pixels.

2. Data Preprocessing: 
The dataset chosen is the FER2013 and with 7 different labels ranging from 0 to 6,
and each label having the following number of instances in the training set:

0=Angry 1=Disgust 2=Fear 3=Happy 4=Sad 5=Suprise 6=Neutral
3995       436     4096   7214   4830    3171       4965

With the number of images extremely high for the Happy label 
(around 3000 higher than the average) and extremely low for the 
disgust label (close to 500), the model is at risk of overfitting
for Happy and underfitting for Disgust. Thus the approach to tackle 
this is data augmentation by adding 3500 Disgust images to our dataset 
and deleting 3000 images from the Happy images.
The added Disgust images could not be taken from the different datasets 
mentionned in the paper since they were not 48x48 in dimension nor grayscale. 
Thus adding the 436 Disgust images repetitively 8 times to the csv file is the 
approach that was taken. I am aware that this will mislead the model as the pictures 
are the same, thus I am working on finding another dataset that fits with FER2013. 
The final result should be:

0=Angry 1=Disgust 2=Fear 3=Happy 4=Sad 5=Suprise 6=Neutral 
3995       3936     4096   4214   4830    3171     4965

3. Machine learning model:

a) The framework from the DeepEmotion 2019 paper that was followed, 
as showed below, is the base for the construction of the convolutional neural network:

''''
#Please refer to page 3 of the following research paper:
Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network
Shervin Minaee, Amirali Abdolrashidi, Expedia Group, University of California, Riverside
''''

The feature extraction part consists of four convolutional layers, each two followed 
by max-pooling layer and rectified linear unit (ReLU) activation function. They are 
then followed by a dropout layer and two fully-connected layers. 
The spatial transformer (specified as the localization network in the figure) consists 
of two convolution layers (each followed by max-pooling and ReLU), and two fully-connected
layers. The implementation of a spatial transformer allows the model to focus on the most 
relevant part of the image, by estimating a sample over the attended region.

The activation function ReLu (Rectified Linear Unit) was chosen over sigmoid and tanh because 
it does not activate all the neurons in our CNN at the same time: if the output of the linear 
transformation is less than 0, the neurons are not activated. This allows the computation to be 
carried out at a higher speed. 

In order to combine the output of neuron cluster at one layer into a single neuron in 
the next layer, a pooling function is used. In this case, Maxpool2D is responsible for 
reducing the spatial size of the convoluted feature, and uses the maximum value from each 
cluster of neurons at the prior layer. It is done to in part to help over-fitting by providing 
an abstracted form of the representation. As well, it reduces the computational cost by reducing 
the number of parameters to learn and provides basic translation invariance to the internal representation.

b) Unfortunately I have not yet tested my model as I am still working on the remaining 
classes that my main will be calling.

c) Yes, at first glance of the proposed framework in the paper, I was unaware of the 
logic behind convolutional networks. After learning about CNNs and understanding what 
weights and biases where, I was unsure of which library to use for image classification. 
I discovered two: Pytorch and TensorFlow, and opted for Pytorch because it seemed to presents 
a more straighforward implementation and was deemed lighter to work with, as opposed to tensorflow. 
I then searched for in depth explanations of the functions used in the framework to understand why 
they were used, and how they affected the classification process, since googling the documentation 
only provided me with what to plug in and where. 

4) Please see section 3, b.

5) My next steps consist in completing the implementation of the framework provided 
by the DeepEmotion2019 paper now that I have gained an in-depth understanding of how 
convolutional neural networks operate. In addition, finding a way to input a live image 
from a web-cam is the second thing on my to do list, with the third being attending the 
workshop held by MAIS to integrate my ML program into a web-app.
